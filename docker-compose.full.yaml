# ============================================================================
# LLM Council - Docker Compose Configuration
# ============================================================================
# This file defines the services for running the LLM Council
# 
# For 2-PC Setup:
# - PC1 (Chairman): Run only the chairman and backend services
# - PC2 (Council): Run only the Ollama service with council models
# 
# See DEPLOYMENT.md for detailed setup instructions
# ============================================================================

version: '3.8'

services:
  # --------------------------------------------------------------------------
  # Backend Service (FastAPI)
  # Should run on PC1 (same machine as frontend for simplicity)
  # --------------------------------------------------------------------------
  backend:
    build:
      context: ./backend
      dockerfile: dockerfile
    container_name: llm-council-backend
    ports:
      - "8000:8000"
    environment:
      - CHAIRMAN_IP=${CHAIRMAN_IP:-ollama-chairman}
      - CHAIRMAN_PORT=${CHAIRMAN_PORT:-11434}
      - CHAIRMAN_MODEL=${CHAIRMAN_MODEL:-qwen2.5:1.5b}
      - COUNCIL_IP=${COUNCIL_IP:-ollama-council}
      - COUNCIL_PORT=${COUNCIL_PORT:-11434}
    volumes:
      - ./backend:/app
      - ./data:/app/data
    depends_on:
      - ollama-chairman
    networks:
      - llm-council-network
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # Frontend Service (React + Vite)
  # Should run on PC1
  # --------------------------------------------------------------------------
  frontend:
    build:
      context: ./frontend
      dockerfile: dockerfile
    container_name: llm-council-frontend
    ports:
      - "5173:5173"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    depends_on:
      - backend
    networks:
      - llm-council-network
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # Chairman Ollama Service (PC1)
  # This service should run on PC1 - handles ONLY synthesis
  # --------------------------------------------------------------------------
  ollama-chairman:
    image: ollama/ollama:latest
    container_name: llm-council-ollama-chairman
    ports:
      - "11434:11434"
    volumes:
      - ollama-chairman-data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    networks:
      - llm-council-network
    restart: unless-stopped
    # GPU support (optional, comment out if not using GPU)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # --------------------------------------------------------------------------
  # Council Ollama Service (PC2)
  # This service should run on PC2 - handles council models
  # --------------------------------------------------------------------------
  ollama-council:
    image: ollama/ollama:latest
    container_name: llm-council-ollama-council
    ports:
      - "11435:11434"  # Different port to avoid conflicts if testing on same machine
    volumes:
      - ollama-council-data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    networks:
      - llm-council-network
    restart: unless-stopped
    # GPU support (optional, comment out if not using GPU)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

# ----------------------------------------------------------------------------
# Networks
# ----------------------------------------------------------------------------
networks:
  llm-council-network:
    driver: bridge

# ----------------------------------------------------------------------------
# Volumes for persistent model storage
# ----------------------------------------------------------------------------
volumes:
  ollama-chairman-data:
    name: llm-council-ollama-chairman
  ollama-council-data:
    name: llm-council-ollama-council
